---
title: 线性模型
published: 2025-10-21
description: 线性模型学习笔记
draft: false
---



# 一、什么是线性模型

## 1、定义

线性模型的通用表达式为：

*y*^=*w*1*x*1+*w*2*x*2+⋯+wpxp*+*b

其中：

- *y*^ 是模型的预测值；
- *x*1,*x*2,…,xp 是输入特征（如房价预测中的卧室数、面积等）；
- *w*1,*w*2,…,wp 是**特征权重**（表示每个特征对预测结果的影响程度）；
- *b* 是**偏置项**（截距，控制模型在无特征输入时的基准输出）。

也可通过向量形式简化为：

y= $\mathbf{w}^\mathrm{T}$ *x+b

（**w**=[*w*1,*w*2,…,wp]T 是权重向量，**x**=[*x*1,*x*2,…,xp]T 是特征向量）

最终输出就是这个y^



然后我们就是要不断调整参数w和b然后让y^越来越解决真实值

## 2、举个例子

| 内容分类       | 具体说明                                                     |
| -------------- | ------------------------------------------------------------ |
| **应用示例**   | 以 “房价预测” 为场景，假设 3 个特征：- *x*1：卧室数量- *x*2：浴室数量- *x*3：居住面积 |
| **模型公式**   | 预测房价的线性表达式：*y*=*w*1*x*1+*w*2*x*2+*w*3*x*3+*b*，其中：- *w*1,*w*2,*w*3：特征权重（需从训练数据中学习）- *b*：偏置项（截距，需从训练数据中学习） |
| **一般化形式** | 对于 *p* 个特征的数据集 **x**=[*x*1,*x*2,...,*x**p*]，线性回归的通用形式为：*y*=⟨**w**,**x**⟩+*b*=*w*1*x*1+*w*2*x*2+...+*w**p**x**p*+*b*，其中 **w**=[*w*1,*w*2,...,*w**p*] 是可训练的权重向量。 |



# 二、损失函数

## 均方误差

对于销量预测为 $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$ ，即模型预测值与真实值之间的平均误差程度，用于衡量损失值 

对于分类为 $$ \text{MSE} = \frac{1}{m} \|\mathbf{o} - \mathbf{y}\|_2^2 = \frac{1}{m} \sum_{i=1}^{m} (o_i - y_i)^2 $$



# 三、线性模型分类

## 1. 核心概念区分

| 任务类型 | 输出特点                       | 示例场景           |
| -------- | ------------------------------ | ------------------ |
| 回归     | 连续值输出（如房价、温度）     | 房价预测、销量预测 |
| 多分类   | 离散类别输出（如类别 0、1、2） | 图像分类、疾病诊断 |

## 2、线性回归用于预测的实现逻辑

- **核心原理**：线性回归通过构建输入特征与连续输出变量之间的线性关系模型来实现预测，形式通常为 *y*^=*w*1*x*1+*w*2*x*2+⋯+wnxn+*b*（其中 *y*^ 是预测值，*x*1,*x*2,…,xn 是输入特征，*w*1,*w*2,…,wn 是权重，*b* 是偏置）。
- **模型训练**：通过最小化均方误差（MSE）来学习最优的权重和偏置。
- **预测逻辑**：将新的输入特征代入训练好的线性模型，计算得到的结果即为预测值。

## 3. 线性回归用于多分类的实现逻辑

- **输出设计**：模型输出为向量，第 *i* 个元素表示 “样本属于类别 *i*” 的置信度分数。
- **模型结构**：为每个类别学习一个线性模型，形式为 *o**i*=⟨**x**,**w***i*⟩+*b**i*（**w***i* 是类别 *i* 的权重向量,bi 是偏置）。
- **标签编码**：标签 **y**=[*y*1,*y*2,...,*y**m*] 采用 “独热编码”，即若真实类别是 *i*，则 yi=1，其余为 0。
- **损失函数**：最小化均方误差（MSE，用置信率减去独热编码然后平方求和求平均)
- **预测逻辑**：选择置信度分数最高的类别



# 四、softmax激活函数

作为一个激活函数，通过非线性变化，将一个向量改变为概率分布

 $$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

比如：z=[4,2,1]经过softmax之后变为[54.598,7.389,2.718]

可以明显看到差距变得非常大，这就让我们更加聚焦于真实的类别，在性能上不关心其他的类别

# 五、交叉墒

作为损失函数，同样为了更加聚焦于真实的类别，在性能上不关心其他的类别，采用了交叉熵作为损失函数

$$ L=-\log \hat{y}_c $$

yc为正确类别的预测概率，通过不断减少L来优化参数



# 六、线性回归模型实现对房价的预测

这里提供两个版本scikit版本和pytorch版本

scikit版本如下：

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

data=fetch_california_housing()

X,Y=data.data,data.target

Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,test_size=0.7,train_size=0.3,random_state=10)

model=LinearRegression()
model.fit(Xtrain,Ytrain)

result=model.predict(Xtest)

accuracy=mean_squared_error(Ytest,result)

print(Ytest,'\n',result)
print(accuracy)
```

scikit版本比较简单，主要是它相当于一个工具箱，对模型已经封装好了，你直接调用就行

pytorch版本:

```python
from torch.nn import Linear
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
import torch
from sklearn.preprocessing import StandardScaler

data=fetch_california_housing()

X,Y=data.data,data.target

#对特征进行标准化处理
scaler = StandardScaler()
X=scaler.fit_transform(X)
#将数据集分为训练集和测试集
Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,test_size=0.7,train_size=0.3,random_state=10)

#将数据转换为张量
Xtrain=torch.tensor(Xtrain,dtype=torch.float32)
Xtest=torch.tensor(Xtest,dtype=torch.float32)
Ytrain=torch.tensor(Ytrain.reshape(-1,1),dtype=torch.float32)#把标签转化为二维列向量，防止与预测结果计算损失函数时维度不匹配触发广播机制
Ytest=torch.tensor(Ytest.reshape(-1,1),dtype=torch.float32)#把标签转化为二维列向量，防止与预测结果计算回归率时维度不匹配触发广播机制

#构造模型
class Model(torch.nn.Module):
    def __init__(self,input_size):
        super().__init__()
        self.linear=Linear(input_size,1)#匹配数据集的特征维度
    
    def forward(self,x):#前向传播
        return self.linear(x)
    
input_size=Xtrain.shape[1]
model=Model(input_size)

#损失函数和优化函数
criterion=torch.nn.MSELoss()
optimizer=torch.optim.SGD(model.parameters(),lr=0.001)

#训练模型
for epoch in range(10000):
    #清空梯度
    optimizer.zero_grad()
    #使用模型进行预测
    pre=model(Xtrain)
    #计算损失函数
    loss=criterion(pre,Ytrain)
    #反向传播
    loss.backward()
    #优化
    optimizer.step()
    if (epoch + 1) % 1000 == 0:
        print(f"Epoch [{epoch+1}/10000], Loss: {loss.item():.6f}")
    

model.eval()#切换为测试模式

with torch.no_grad():  # 禁用梯度计算
    # 在测试集上预测
    result = model(Xtest)
    
    # 计算测试集的损失函数MSE（均方误差）
    test_mse = criterion(result, Ytest).item()#.item()获取张量里面的内容做标量
    
    # 计算R²评分（需转换为numpy数组，因为sklearn的r2_score不直接支持张量）
    from sklearn.metrics import r2_score
    test_r2 = r2_score(Ytest.numpy(),result.numpy())
    print(f"Test MSE: {test_mse:.6f}")
    print(f"Test R² Score: {test_r2:.6f}")
```

pytorch相对来讲比较麻烦，主要是因为pytorch必须要你自己来搭建模型，并且了解整个流程
