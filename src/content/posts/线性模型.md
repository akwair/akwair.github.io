---
title: 线性模型
published: 2025-10-20
description: 线性模型学习笔记
draft: false
---



# 一、什么是线性模型

## 1、定义

线性模型的通用表达式为：

*y*^=*w*1*x*1+*w*2*x*2+⋯+wpxp*+*b

其中：

- *y*^ 是模型的预测值；
- *x*1,*x*2,…,xp 是输入特征（如房价预测中的卧室数、面积等）；
- *w*1,*w*2,…,wp 是**特征权重**（表示每个特征对预测结果的影响程度）；
- *b* 是**偏置项**（截距，控制模型在无特征输入时的基准输出）。

也可通过向量形式简化为：

y= $\mathbf{w}^\mathrm{T}$ *x+b

（**w**=[*w*1,*w*2,…,wp]T 是权重向量，**x**=[*x*1,*x*2,…,xp]T 是特征向量）

最终输出就是这个y^



然后我们就是要不断调整参数w和b然后让y^越来越解决真实值

## 2、举个例子

| 内容分类       | 具体说明                                                     |
| -------------- | ------------------------------------------------------------ |
| **应用示例**   | 以 “房价预测” 为场景，假设 3 个特征：- *x*1：卧室数量- *x*2：浴室数量- *x*3：居住面积 |
| **模型公式**   | 预测房价的线性表达式：*y*=*w*1*x*1+*w*2*x*2+*w*3*x*3+*b*，其中：- *w*1,*w*2,*w*3：特征权重（需从训练数据中学习）- *b*：偏置项（截距，需从训练数据中学习） |
| **一般化形式** | 对于 *p* 个特征的数据集 **x**=[*x*1,*x*2,...,*x**p*]，线性回归的通用形式为：*y*=⟨**w**,**x**⟩+*b*=*w*1*x*1+*w*2*x*2+...+*w**p**x**p*+*b*，其中 **w**=[*w*1,*w*2,...,*w**p*] 是可训练的权重向量。 |



# 二、损失函数

## 均方误差

对于销量预测为 $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$ ，即模型预测值与真实值之间的平均误差程度，用于衡量损失值 

对于分类为 $$ \text{MSE} = \frac{1}{m} \|\mathbf{o} - \mathbf{y}\|_2^2 = \frac{1}{m} \sum_{i=1}^{m} (o_i - y_i)^2 $$



# 三、线性模型分类

## 1. 核心概念区分

| 任务类型 | 输出特点                       | 示例场景           |
| -------- | ------------------------------ | ------------------ |
| 回归     | 连续值输出（如房价、温度）     | 房价预测、销量预测 |
| 多分类   | 离散类别输出（如类别 0、1、2） | 图像分类、疾病诊断 |

## 2、线性回归用于预测的实现逻辑

- **核心原理**：线性回归通过构建输入特征与连续输出变量之间的线性关系模型来实现预测，形式通常为 *y*^=*w*1*x*1+*w*2*x*2+⋯+wnxn+*b*（其中 *y*^ 是预测值，*x*1,*x*2,…,xn 是输入特征，*w*1,*w*2,…,wn 是权重，*b* 是偏置）。
- **模型训练**：通过最小化均方误差（MSE）来学习最优的权重和偏置。
- **预测逻辑**：将新的输入特征代入训练好的线性模型，计算得到的结果即为预测值。

## 3. 线性回归用于多分类的实现逻辑

- **输出设计**：模型输出为向量，第 *i* 个元素表示 “样本属于类别 *i*” 的置信度分数。
- **模型结构**：为每个类别学习一个线性模型，形式为 *o**i*=⟨**x**,**w***i*⟩+*b**i*（**w***i* 是类别 *i* 的权重向量,bi 是偏置）。
- **标签编码**：标签 **y**=[*y*1,*y*2,...,*y**m*] 采用 “独热编码”，即若真实类别是 *i*，则 yi=1，其余为 0。
- **损失函数**：最小化均方误差（MSE，用置信率减去独热编码然后平方求和求平均)
- **预测逻辑**：选择置信度分数最高的类别



# 四、softmax激活函数

作为一个激活函数，通过非线性变化，将一个向量改变为概率分布

 $$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

比如：z=[4,2,1]经过softmax之后变为[54.598,7.389,2.718]

可以明显看到差距变得非常大，这就让我们更加聚焦于真实的类别，在性能上不关心其他的类别

# 五、交叉墒

作为损失函数，同样为了更加聚焦于真实的类别，在性能上不关心其他的类别，采用了交叉熵作为损失函数

$$ L=-\log \hat{y}_c $$

yc为正确类别的预测概率，通过不断减少L来优化参数

