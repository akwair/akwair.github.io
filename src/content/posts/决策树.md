---
title: 决策树学习笔记
published: 2025-10-19
description: 决策树学习笔记
draft: false
---

# 决策树学习笔记

决策树作为监督学习的一种模型，正如他的名字一样是通过不断对一个数据进行判断筛选，最后返回结果，相当于if-else的集成，但是与if-else不同的是，他不需要人工的预设，他会通过算法自动选择最有区分度的的特征和划分阈值

## 一、核心原理

决策树的构建过程是**递归地选择最优特征进行数据划分**，直到满足停止条件（如节点样本数过少、纯度足够高）。

- 根节点是所有的样本
- 内部节点是对某个特征的判断，类似于（if age>30）
- 叶子节点就是所有的输出结果

## 二、优缺点

### 优点

- **可解释性强**，可以看出返回逻辑，常用于银行判断是否放贷。

- **兼容多类型特征**：可同时处理数值型（如年龄、收入）和分类型（如性别、职业）特征，无需额外的特征编码或缩放。

### 缺点

- **鲁棒性差**：单棵决策树对数据变化非常敏感，微小的数据波动可能导致树结构大幅改变（通常通过集成学习如随机森林来解决）。
- **易过拟合**：树的结构过于复杂时，会对训练数据过度拟合，泛化能力下降（可通过剪枝、限制树深度等方式缓解）。
- **计算并行性差**：决策树的构建是串行过程，难以在计算上进行高效并行化，大规模数据训练时效率受限。



## 三、随机森林

在机器学习中，**随机森林（Random Forest）** 是一种强大的**集成学习模型**，由多棵决策树 “集体决策” 而成，既保留了决策树可解释性的优点，又解决了单棵树易过拟合、不稳定的问题。

随机森林是**Bagging**模式的典型代表

### 核心原理

随机森林通过 **“随机采样 + 多树集成”** 实现：

1. **样本随机采样（Bootstrap）**：从训练集中有放回地随机抽取多个子样本集（每个子样本集的大小与原训练集相同，约 63.2% 的样本会被选中，剩下的 36.8% 为 “袋外样本”）。
2. **特征随机选择**：每棵树在分裂时，仅从随机选取的部分特征中选择最优分裂特征（而非全量特征）。
3. **多树并行训练**：用每个子样本集和对应特征集训练一棵决策树，所有树独立并行训练。
4. 集体决策：
	- 分类任务：多棵树投票，得票最多的类别为最终结果。
	- 回归任务：多棵树预测值的平均为最终结果。

### 优势

- **泛化能力强**：通过多树集成和随机采样，大幅降低了单棵树过拟合的风险，在新数据上的表现更稳定。
- **鲁棒性高**：对噪声和异常值不敏感（多树投票会抵消个别树的错误）。
- **支持多任务**：既可以做分类（如客户流失预测），也可以做回归（如房价预测），还能评估特征重要性。
- **可解释性较好**：虽然是多树集成，但可以通过单棵树的结构或特征重要性分析理解决策逻辑。

### 劣势

- 计算效率相对单个决策树下降

## 四、bagging策略与boosting策略

 是集成学习的两大核心策略：

### 一、定义

| 策略     | 核心定义                                                     | 核心思想                             |
| -------- | ------------------------------------------------------------ | ------------------------------------ |
| Bagging  | Bootstrap Aggregating（自助聚合），通过 “有放回采样 + 并行训练 + 平等聚合” 组合基模型 | 降低单模型的**方差**（避免过拟合）   |
| Boosting | 迭代提升，通过 “串行训练 + 修正错误 + 加权聚合” 组合基模型，让后模型弥补**前模型**的不足 | 降低单模型的**偏差**（提升拟合能力） |

### 二、差异

| 对比维度        | Bagging                                                      | Boosting                                                     |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1. 训练顺序     | **并行训练**：所有基模型独立同时训练，互不依赖               | **串行训练**：基模型按顺序训练，后模型依赖前模型的错误结果   |
| 2. 样本处理     | - 有放回采样，每个子样本集独立- 所有样本初始权重相同         | - 无采样（或少量采样），全量样本参与每轮训练- 样本权重动态调整：前模型预测错误的样本权重升高，让后模型重点学习 |
| 3. 模型权重     | **平等权重**：所有基模型地位相同，分类用 “投票”、回归用 “平均” | **加权权重**：表现好的基模型（预测误差小）权重更高，对最终结果影响更大 |
| 4. 过拟合风险   | 风险低：多模型并行抵消噪声，基模型越复杂，Bagging 效果越明显 | 风险较高：串行修正易过度关注训练集错误（噪声），需通过正则化（如学习率、剪枝）控制 |
| 5. 对异常值敏感 | 不敏感：并行投票稀释异常值的影响                             | 敏感：错误样本权重持续升高，异常值可能被反复关注，影响整体性能 |



## 五、代码实现对鸢尾花的分类

这里给一个用代码实现对鸢尾花分类的决策树模型,对于函数有不理解的地方建议查看scikit官方的api文档[API Reference — scikit-learn 1.7.2 documentation](https://scikit-learn.org/stable/api/index.html)

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#经典的鸢尾花分类问题
data=load_iris()#首先下载数据集
#接着分别储存特征和标签
X=data.data
Y=data.target
#将其随机分为训练集和测试集，注意，这里必须将特征和标签分开放入，确定随机种子可以确保分的结果一致
#这里也可以采取其他的随机分配函数
Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,
                                           train_size=0.7,
                                           test_size=0.3,
                                           random_state=10)

#创建一个分类的决策树，通过确定随机种子给确保可以复现
model=DecisionTreeClassifier(random_state=10)
#训练决策树
model.fit(Xtrain,Ytrain)
#通过决策树对测试机进行预测
result=model.predict(Xtest)
#计算准确率
accuracy = accuracy_score(Ytest,result) 
print(Ytest,'\n',result)
print(accuracy)
```

