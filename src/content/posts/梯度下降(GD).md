---
title: 梯度下降
published: 2025-10-20
description: 梯度下降学习笔记
draft: false
---

# 一、什么是梯度下降

前面我们说过，在线性模型中，我们通过不断调整参数来不断优化线性方程，使其结果更加拟合我们的真实类别（或者更接近准确值），而如何反映拟合效果呢？则是通过激活函数使结果更加聚焦于真实的类别，减少其他类别的影响，再通过formax损失函数来评估损失值，每次调参完成后通过损失值来反应拟合效果

但是你会不会发现缺了些什么？就是我们总说调参，那我们怎么调参呢？

我们肯定要将损失函数往降低最快的方向变化（梯度方向）

所以这里我们就要用到梯度下降算法，作为一种优化算法，他分为三类：1、批量梯度下降（BSG）2、随机梯度下降（SGD）3、小批量梯度下降(MBGD)

| 类型                       | 核心特点                                           | 优点                                 | 缺点                                     |
| -------------------------- | -------------------------------------------------- | ------------------------------------ | ---------------------------------------- |
| **批量梯度下降（BGD）**    | 每次使用**全部训练数据**计算梯度（全部数据的loss） | 收敛稳定，能找到全局最优（凸函数下） | 数据量大时，计算速度极慢，内存占用高     |
| **随机梯度下降（SGD）**    | 每次仅使用**1 个随机样本**计算梯度(单个数据的loss) | 计算速度快，适合大数据               | 梯度波动大，收敛路径曲折，易陷入局部最优 |
| **小批量梯度下降（MBGD）** | 每次使用**一小批样本**（如 32/64 个）计算梯度      | 平衡速度与稳定性，是工业界常用方式   | 需要手动调整 “批量大小” 这一超参数       |

# 二、简述数学原理

## 1、数学公式

$$ \mathbf{\theta}_{t+1} = \mathbf{\theta}_t - \eta_t \cdot \nabla_{\mathbf{\theta}_t} J(\mathbf{\theta}_t)$$ 

## 2、参数解释

梯度这个词并不陌生，他是多元函数中下降（上升）最快的方向，你要注意这里的梯度是损失函数关于模型参数的梯度$$ \nabla L = \left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, \ldots, \frac{\partial L}{\partial w_n}, \frac{\partial L}{\partial b} \right)$$ 是一个包含了所有参数偏导在参数值点的取值的向量,即公式中的$$  \nabla_{\mathbf{\theta}_t} J(\mathbf{\theta}_t)$$ 

此外我们还要确定我们到底一次变化变化多长，即我们沿梯度方向走多远，这个参数就是$$ \eta_t$$ 

参数的迭代肯定是下一代相对于上一代的迭代，那么$$ w_t$$ 代表着第t代，$$ w_{t+1}$$ 代表第t+1代



# 三、注意事项

在梯度下降中，梯度下降的函数参数对优化效率及质量影响是十分大的,所以可以多次调参然后选取最优



