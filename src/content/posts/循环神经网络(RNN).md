---
title: 循环神经网络（CNN）
published: 2025-10-24
description: 循环神经网络学习笔记
draft: false
---

在前面我们提到了多层感知机和卷积神经网络，但是对于自然语言这种序列数据，他们是无法处理的，所以出现了循环神经网络

# 一、RNN 的核心结构：循环单元与记忆传递

RNN 的基本单元是**循环神经元**，其结构可简化为：

```plaintext
输入序列：x₁ → x₂ → x₃ → ... → xₜ（t为时间步）
    ↓      ↓      ↓           ↓
循环单元：[ ] → [ ] → [ ] → ... → [ ]
    ↓      ↓      ↓           ↓
输出序列：h₁ → h₂ → h₃ → ... → hₜ（hₜ为t时刻的隐藏状态，即“记忆”）
```

- **核心逻辑**：在每个时间步 *t*，循环单元接收当前输入$x_t$ 和上一时间步的隐藏状态 $h_{t-1}$（即 “历史记忆”），输出当前隐藏状态 $h_t$：

	$h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$

	其中：

	- $W_{xh}$是输入到隐藏状态的权重(输入的权重),$W_{hh}$是隐藏状态到自身的权重（体现 “记忆传递”），$b_h$ 是偏置；
	- *f* 是非线性激活函数（如 tanh、ReLU）。

- **输出层**：若需要预测，可在每个时间步添加输出层，基于 *h**t* 生成预测结果 *y**t*：

	$y_t = g(W_{hy} h_t + b_y)$

	（*g* 是输出激活函数，如 softmax 用于分类）。

# 二、损失函数

自然语言是离散的符号（词或字符），需先转换为数值形式：

- **词表与索引**：将所有可能的词 / 字符构建成 “词表”，每个词对应一个唯一索引（如 “你”→0，“好”→1，“！”→2…）。
- **One-hot 编码**：用长度为词表大小的向量表示每个词，对应索引位置为 1，其余为 0（如 “好”→`[0,1,0,...]`）。
- **词嵌入**：更高效的表示方式，将词映射到低维连续向量（如 “好”→`[0.1, 0.5, -0.3]`），由模型学习得到。

假设 RNN 生成的是一个词序列$\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_T$，真实序列是 $ y_1$,$y_2$,...,$ y_t $（每个$ y_t$是词的索引或 one-hot 向量），损失计算步骤如下：

#### 1. 交叉熵损失（逐词累加）

对每个时间步*t*，计算生成词与真实词的交叉熵，再求和：

- $L = -\sum_{t=1}^{T} \sum_{i=1}^{V} y_{t,i} \cdot \log(\hat{y}_{t,i})$
- *V*是词表大小；
- $y_{t,i}$,是真实词的 one-hot 编码（只有一个位置为 1）；
- $\hat{y}_1$,*i*是 RNN 预测的第*t*步第*i*个词的概率。

# 三、注意点

当序列过长会导致早期记忆因为梯度爆炸而消失,此模型与CNN等有很大不同